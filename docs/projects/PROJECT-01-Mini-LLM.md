\# PROJECT 01 - Mini-LLM from Scratch

\*\*Dates:\*\* 2026-09-10 -> 2026-12-02 (12 weeks)



\## Objective

Train and analyze a small language model with reproducible workflow.



\## Core Resources

\- nanoGPT

\- TinyStories (paper + dataset)

\- D2L and PyTorch tutorials



\## Weekly Outline

1\. Reproduce base training

2\. Build TinyStories subset pipeline

3\. Train model A

4\. Train model B (depth/width variation)

5\. Validation and loss curves

6\. Sampling + error analysis

7\. Ablation: context length

8\. Ablation: batch size or LR

9\. Stable final run

10\. Code cleanup

11\. Technical report draft

12\. Public release



\## Done Definition

\- Reproducible repo with `train.py`, `eval.py`, README

\- Training/validation curves

\- Short report (6â€“10 pages) with decisions, failures, limits



